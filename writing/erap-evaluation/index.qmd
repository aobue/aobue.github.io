---
title: "Does Emergency Rental Assistance Prevent Homelessness?"
author: "Alex Bue"
date: "2026-02-01"
format: html
---

If you think the hardest part of life is the what-could-have-been's of love, career, and family, you're wrong: this is the hardest part of causal inference. The fundamental problem of causal inference is that we live in a single- rather than multi-verse, "that we can observe only one of the potential outcomes for a particular subject," as Guido Imbens and Donald Rubin put it. So statistics, like our insecurities, devises variously clever ways of imagining counterfactuals. Generally, we do this in order to understand causal effects: the difference between what happened and what would have happened otherwise — between how you felt after that aspirin and how you would have felt without it.

Causal effects are the preeminent measure of impact in social policy, following the general zeal for randomized controlled trials (RCTs) among wonks and academics after Abhijit V. Banerjee and Esther Duflo's *Poor Economics*. RCTs are desirable because they tidily handle the variation between people that might otherwise distort the effect of some treatment. Treatments are randomly assigned — or randomly assigned within some strata — and so we expect on average the control group to be similar, or not-different in a systematic way. This allows us to compare the effect of interventions without confounding.

But we often care about policies before we have studied them using RCTs, so it is necessary to conduct observational analysis that approximates the what-would-have-been using other methods. This is especially hard in settings like homeless services if our goal is understanding what causes a person to become homeless, which also means understanding what does not cause a person to become homeless. American censuses don't track people before and after they become homeless; instead, homelessness data comes to exist precisely when a person is already homeless or seeking help for imminent housing loss. We never observe the non-homeless counterfactual in the same sample — only who is already seeking help. For this reason every single statement about the "causes" of homelessness should be regarded with extreme skepticism. What can be analyzed is the causal impact of rental assistance (or the intent to provide rental assistance) on homelessness for the small sub-population of households that interact with social services.

This is precisely what Focus Strategies attempted to do in its recently-published impact evaluation of San Francisco's Emergency Rental Assistance Program (ERAP). ERAP provides low income households determined to be at high risk of homelessness with financial assistance for past due rent or move-in costs. Focus Strategies did not administer an RCT; instead they compared the group of eligible households who did receive assistance and the group of eligible households who dropped before they received assistance. The counterfactual here was studied using San Francisco's homeless management information system (HMIS — or the One System in the City), another imperfect measure of homelessness. It's possible households who did not enter the local homeless database entered it elsewhere, in another county, or that households became homeless and didn't interact with local service providers.

The ERAP impact evaluation headline, shared across Linkedin by professionals in the homelessness space, is: "Program participants were 40% less likely to experience homelessness in the 12 months after receiving SF ERAP assistance." A credulous and, it must be said, implicated group of people in San Francisco City Government and nonprofits applauded the result: "Homelessness prevention works!"

There are two problems here.

The first is causal. Unfortunately, phrases like "DAG" and logistic regression are mistaken for evidence themselves; but the usage of certain techniques does not guarantee a causal impact or even the creation of "evidence". DAGs are a theoretical tool, not an empirical one. Logistic regression takes a cloud of data — say, data about age, income, race, sex, and whether or not the person received ERAP — and spits out a probability of becoming homeless in a way that is relatively interpretable: a decrease in, for example, income yields a constant increase in the (natural-logarithmic) odds ratio. The rigor of a causal claim comes from the research design, not the regression output. It is somewhat misleading for Focus Strategies to call DAGs and logistic regression "causal inference methods" when really they are modeling techniques indifferent to their application; to do so conflates the tool with the work it's supposed to do. Behind this slight obfuscation Focus Strategies hides the real problem.

The validity of the causal effect of ERAP depends upon unconfoundedness of the comparison between the treatment group — those who receive assistance — and the control group — those who were eligible but did not receive assistance. Are the groups of clients who drop out really comparable to those who do not?

Program attrition is common in social service settings, as case managers lose touch with clients who are hard-to-reach and stretched thin (incidentally, the same thing could be said by clients about most case managers). Focus Strategies controls for observable differences in their regression: the datapoints, usually demographic, created at enrollment. They do not seem to control for unobserved differences, leaving a vacuum which, in the absence of evidence, you fill with a question: not how lapsing from the program could be immaterial to later homelessness — how couldn't it be?

If one of two prospective clients, identical with respect to age and race and sexuality and income and education and zip code, disappears after being approved for ERAP, you do not naturally assume that, six months later, these two individuals' outcomes would have been identical except for the provision of assistance. You assume there is some influence in their life not captured by the data but meaningful enough to derail a desperate person from receiving funds to prevent their eviction. Focus Strategies recognizes this: "there may be inherent differences in the comparison group that increase their likelihood of entering the homeless response system". This is not a technical detail, however; it's the foundation on which causal inference rests. We can settle for correlations, but at the cost of answering a different question — not the effect of ERAP on all eligible households, but the effect on households who were somehow disposed to complete the process.

I wonder if the impact evaluation audience actually read the study, or even the summary of the study, which continues: "The finding is significant but modest, as most participants would not experience homelessness. Receiving assistance reduced the chance of becoming homeless from 8% to 4.8%." Even setting aside the causal issues — granting the 8% and 4.8% numbers at face value — there is a more basic question about what it means for a program to "work." This is the second problem.

In computer science theory, or formal logic, or whatever, conditional statements are said to be "vacuously" true if the antecedent cannot be satisfied. I do not have children and do not own cats, so the statement "I love my children" is vacuously true. Similarly, the claim that ERAP works is the conditional: "If a participant is headed for homelessness, then ERAP prevents it". But this is vacuously true for the 92% of people who were never going to become homeless anyway, according to the study. The statement cannot be proven false, so by default, it is considered true.

When we talk about programs working, what we mean and hope is they were the difference between a better and worse outcome: ERAP works because, had it not been for emergency rental assistance, families would become homeless. We almost certainly do not mean they work because the circumstances they mean to prevent rarely happen — we do not mean they work vacuously.
