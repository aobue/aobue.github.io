---
title: "Housing Investments ≠ Homelessness Reductions"
author: "Alex Bue"
date: "2025-12-31"
format: html
---

When I graduated from Catholic middle school to Catholic high school, my mother bought me a key ring in hopeful anticipation that I would get a driver's license, and my father wrote me a timely note transferring decades of manful wisdom: "Never trust a skinny cook." He should also have said: Never trust, or almost never trust, statistics about homelessness, in particular statistics about homelessness claiming to evince causality.

Jonathan Russell, Director of Housing and Homelessness for Alameda County, is confident his department's policies work. In a recent LinkedIn post, he shared results from the county's Together 2026 Community Plan: after $254 million in housing capital funding in fiscal year 2022, the Point in Time (PIT) count dropped and "housing outcomes" improved. Russell calls this "pretty straightforward" — spending caused the improvement.

## The claim

![Russell's LinkedIn post claiming housing investments caused homelessness reductions](russell-post.png){width=450}

But two things would need to be true for Russell's argument to hold. First, the PIT decrease must be larger than we'd expect by chance. Second, if it is, then the decrease must have been caused by spending and not something else.

The PIT count is disliked by homelessness professionals because it's infrequent and poorly standardized. But its inadequacies don't stop policy-makers from citing it when convenient — as Russell does here. Between 2022 and 2024, Alameda County's count went from 9,747 to 9,450: a 3% change. This would only be significant if the PIT were a census that captured every homeless person. It isn't. Hundreds of volunteers fan out early in the morning across zip codes; homeless people are often peripatetic; someone is always away from his usual spot when the count happens. Other counties, like Los Angeles, acknowledge this randomness by reporting sampling error. Alameda does not.

## Is the decrease statistically significant?

At first, Russell's claim seems defensible. Each count is generated by an unknown Poisson process, and we can compare the difference using standard statistical tests under the assumption of independence: knowing the 2022 count does not inform our estimate of the 2024 count. First, note the sum of independent Poisson processeses is itself a Poisson distribution:

$$
X_1+X_2 \sim \operatorname{Poisson}\left(\lambda_1+\lambda_2\right)
$$

We're interested in

$$
P\left(X_1=k \mid X_1+X_2=n\right)=\frac{P\left(X_1=k\right) \cdot P\left(X_2=n-k\right)}{P\left(X_1+X_2=n\right)}
$$

Or the probability of drawing some number of events conditional on the sum of both draws. Substituting the Poisson PMFs:

$$
=\frac{\frac{\lambda_1^k e^{-\lambda_1}}{k!} \cdot \frac{\lambda_2^{n-k} e^{-\lambda_2}}{(n-k)!}}{\frac{\left(\lambda_1+\lambda_2\right)^n e^{-\left(\lambda_1+\lambda_2\right)}}{n!}}
$$

Simplifies to:

$$
=\binom{n}{k}\left(\frac{\lambda_1}{\lambda_1+\lambda_2}\right)^k\left(\frac{\lambda_2}{\lambda_1+\lambda_2}\right)^{n-k}
$$

So:

$$
X_1 \mid\left(X_1+X_2=n\right) \sim \operatorname{Binomial}(n, 0.5)
$$

Plugging in our PIT count numbers and their total:

$$
X_1 \mid\left(X_1+X_2=19,197\right) \sim \operatorname{Binomial}(19197,0.5)
$$

Using known properties of mean and variance:

$$
\mu=n p=19197 \times 0.5=9598.5
$$

$$
\sigma^2=n p(1-p)=19197 \times 0.5 \times 0.5=4799.25
$$

$$
\sigma=\sqrt{4799.25}=69.28
$$

So, given our two counts, a simple measure of statistical significance is whether 2024's difference is two standard deviations above or below our expected count of 9596 (with known population variance under the null — not a t-test):

$$
z=\frac{9747-9598.5}{69.28}=\frac{148.5}{69.28}=2.14
$$

The corresponding p-value is approximately 0.032 — significant and therefore contravening the null hypothesis that the true rate parameters in 2022 and 2024 are identical. The conditional binomial test is more internally consistent than a naive z-test comparing 2024's count to 2022's baseline: rather than treating 2022's count as the true rate and 2024's as a random draw from it, the conditional binomial test regards both counts as random and asks whether their split of the total is consistent with equal underlying rates.

## The problem: overdispersion

The problem isn't that we've assumed independence but that we've assumed the rate parameter is known exactly. We treated 2022's count as ground truth, but it's also a noisy observation. Under the assumption that both years were generated by the same Poisson process with rate 9,599, it would be unlikely to observe either count — both are more than one standard deviation from the mean. The data is *overdispersed* relative to the model.

Instead of assuming the rate is known, we can model it as a random variable drawn from a distribution. This is sometimes called a Gamma-Poisson or negative binomial model. Higher variance means a flatter distribution, reflecting greater uncertainty about what the true count actually is.

## Accounting for uncertainty

Now the observed count depends on the rate, which is itself uncertain. To find the total variance, we use the law of total variance, which decomposes it into two sources — randomness given the parameter, and randomness in the parameter itself:

$$
\operatorname{Var}(X) = \mathrm{E}[\operatorname{Var}(X \mid \lambda)] + \operatorname{Var}(\mathrm{E}[X \mid \lambda])
$$

For a Poisson, mean and variance both equal the rate parameter, so:

$$
\mathrm{E}[\operatorname{Var}(X \mid \lambda)] = \mathrm{E}[\lambda] = \lambda_0
$$

$$
\operatorname{Var}(\mathrm{E}[X \mid \lambda]) = \operatorname{Var}(\lambda) = \sigma_\lambda^2
$$

Therefore:

$$
\operatorname{Var}(X) = \lambda_0 + \sigma_\lambda^2
$$

The total variance has two components: the baseline rate (Poisson sampling noise — who happened to be visible on count night) and the parameter uncertainty (what the true rate actually is). A natural way to express this uncertainty is in terms of how many people might have been missed. If the true count could plausibly differ from the baseline by plus or minus U people, then a 95% confidence interval implies:

$$
\sigma_\lambda = \frac{U}{1.96}
$$

With this parameterization, the standard error becomes:

$$
\mathrm{SE}=\sqrt{\lambda_0+\sigma_\lambda^2}=\sqrt{9,747+\frac{U^2}{3.84}}
$$

And the test statistic:

$$
z=\frac{-297}{\sqrt{9,747+U^2 / 3.84}}
$$

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
library(dplyr)
library(patchwork)

# Observed counts
x_2022 <- 9747
x_2024 <- 9450
diff <- x_2024 - x_2022

# True rate centered at 2022 count
lambda_0 <- x_2022

# Vary the "uncounted" people from 0 to 2000
uncounted <- seq(0, 2000, by = 10)

# Convert to standard deviation of lambda
sigma_lambda <- uncounted / 1.96

# Compute variance of observed count under gamma-poisson
var_x <- lambda_0 + sigma_lambda^2
se_x <- sqrt(var_x)

# Z-score and p-value
z <- diff / se_x
p_value <- 2 * pnorm(z)

# Create data frame
df <- data.frame(
  uncounted = uncounted,
  pct_uncounted = 100 * uncounted / lambda_0,
  p_value = p_value
)

# Find crossover point
crossover_05 <- df$uncounted[which.min(abs(df$p_value - 0.05))]
crossover_05_pct <- 100 * crossover_05 / lambda_0

# Common theme
common_theme <- theme_minimal(base_size = 11) +
  theme(
    plot.title = element_text(face = "bold", size = 10),
    panel.grid.minor = element_blank()
  )

# Panel A: People
p1 <- ggplot(df, aes(x = uncounted, y = p_value)) +
  geom_line(linewidth = 1, color = "#2c3e50") +
  geom_hline(
    yintercept = 0.05,
    linetype = "dashed",
    color = "#e74c3c",
    linewidth = 0.7
  ) +
  geom_vline(
    xintercept = crossover_05,
    linetype = "dotted",
    color = "gray50",
    linewidth = 0.7
  ) +
  scale_y_continuous(limits = c(0, 0.5), breaks = seq(0, 0.5, by = 0.1)) +
  scale_x_continuous(breaks = sort(c(seq(0, 2000, by = 500), crossover_05))) +
  labs(
    title = "A. People potentially missed",
    x = "± people potentially uncounted",
    y = "P-value"
  ) +
  common_theme

# Panel B: Percent
p2 <- ggplot(df, aes(x = pct_uncounted, y = p_value)) +
  geom_line(linewidth = 1, color = "#2c3e50") +
  geom_hline(
    yintercept = 0.05,
    linetype = "dashed",
    color = "#e74c3c",
    linewidth = 0.7
  ) +
  geom_vline(
    xintercept = crossover_05_pct,
    linetype = "dotted",
    color = "gray50",
    linewidth = 0.7
  ) +
  scale_y_continuous(limits = c(0, 0.5), breaks = seq(0, 0.5, by = 0.1)) +
  scale_x_continuous(
    breaks = sort(c(seq(0, 20, by = 5), crossover_05_pct)),
    labels = function(x) paste0(round(x, 1), "%")
  ) +
  labs(
    title = "B. Percent of count potentially missed",
    x = "± % of 2022 count potentially uncounted",
    y = "P-value"
  ) +
  common_theme

# Combine
combined <- p1 +
  p2 +
  plot_annotation(
    title = "How many people would need to be missed in the 2022 PIT for the 2022–2024 decline to lose significance?",
    theme = theme(plot.title = element_text(face = "bold", size = 12))
  )

print(combined)
```

Russell's argument depends on the question of whether 2.4% undercounting — 230 people — is implausible. Of course it isn't.

## Causation is weak

Even if the decrease were real, Russell's causal claim would remain weak. Causation requires a counterfactual: some comparable place or time that did not receive similar investments. County-level comparisons are difficult because counties differ in all the ways you can readily imagine. Nearby counties that might be comparable — Contra Costa, for example — suffer from interference: changes in Alameda may have spillover effects.

Should "cause" be purged from policy-makers' vocabulary? No. But we should expect more from homelessness experts than "it's pretty straightforward." I worked as a case manager for two years and managed programs for two more. I've been around homelessness ever since — domestic and international, in graduate school and professional roles. I'm from Oakland and live in San Francisco. The story on the ground is different from the story in LinkedIn posts. Things won't get better until we're clear about what's known and unknown.
